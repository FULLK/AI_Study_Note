
@[toc]
# 拟合
过拟合：参数（特征）过多（理解为考虑很多因素)或者说过多专注于原来的训练数据，导致模型过于复杂
欠拟合：参数太少，太不专注于原来的训练数据，导致模型过于简单

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/94243f6057d44084bb9ce71ceab0557c.png)

随着模型复杂度增加（可以认为与训练数据的拟合程度）训练误差越来越小，泛化误差（就是与测试数据的误差）先小后大

复杂度的低中高分别对应 欠拟合 拟合 过拟合


# 正则化
正则化的方法通常是在损失函数中添加一个正则化项，这个正则化项会根据模型参数的大小来惩罚模型的复杂度
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/0529cc39e4624f4d919156c652686411.png)

# 正则项
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/c07458aeff6f4262857ba6c9a9908177.png)
L1 和 L2 正则化之所以会导致不同的权重效果，主要是由于它们对权重施加的惩罚方式不同（就是让权重变化的方式）
### L1 正则化（Lasso）

L1 正则化在损失函数中加入了权重绝对值的和作为惩罚项。这意味着它会惩罚较大的权重值，并且对于较小的权重值也会产生显著的影响。因为它是基于绝对值的惩罚，所以当优化过程试图最小化整个损失函数时，它倾向于将一些权重直接缩小到零。这是因为：

- 对于非常小的权重 \( w \)，L1 惩罚 \( |w| \) 与 \( w \) 成线性关系，即使 \( w \) 接近于0，其导数仍然是一个非零常数。
- 这导致了优化算法在更新这些权重时有持续的压力使其进一步趋近于0，最终可能达到完全为0的情况。
- 一旦某个权重被设为0，由于L1惩罚不再对其产生影响（因为0的绝对值还是0），该权重就保持不变，相当于从模型中移除了相应的特征。

### L2 正则化（Ridge）

另一方面，L2 正则化在损失函数中加入了权重平方和的惩罚项。这会产生以下效果：

- 对于任何非零权重 \( w \)，L2 惩罚 \( w^2 \) 总是正的，并且随着 \( w \) 的增大而迅速增加。
- 但是，对于接近零的 \( w \)，\( w^2 \) 增长得非常缓慢，因此L2惩罚对这些小权重的影响相对较小。
- 在优化过程中，L2 惩罚会对所有非零权重施加一个趋向于减小但不为零的压力，使得它们都变得比较小，但通常不会变成零。

简而言之，L1 正则化通过绝对值惩罚鼓励稀疏解（即许多权重为0），而L2 正则化通过平方惩罚鼓励所有权重的小值分布，而不是让它们变为零。这种差异来源于两者数学形式的不同以及它们如何影响梯度下降等优化算法的行为。


# 多元线性回归的正则化回归形式
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/6fa1b193f52049748de0aa7d1afba84d.png)
# 代码
[https://github.com/FULLK/AI_Study/blob/main/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%AD%A3%E5%88%99%E5%8C%96%E6%8A%80%E6%9C%AF/20newsgroups_logistic_regressuib.py](https://github.com/FULLK/AI_Study/blob/main/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%AD%A3%E5%88%99%E5%8C%96%E6%8A%80%E6%9C%AF/20newsgroups_logistic_regressuib.py)