@[toc]
# 强化学习（Reinforcement Learning, RL）
强化学习是一种机器学习的范式，智能体（Agent）通过与环境（Environment）的不断交互，学习如何在不同状态下采取最优行动，以 最大化长期累积奖励（Cumulative Reward）。强化学习的核心思想是基于试错（Trial-and-Error）机制，不断调整策略以提高行为效果。
## 基本组成

1. **智能体（Agent）**：
   - 决策的主体。
   - 负责根据当前状态选择动作，并通过学习不断优化策略，目标是最大化累积奖励。

2. **环境（Environment）**：
   - 智能体所在的交互空间。
   - 智能体的行动会改变环境的状态，同时环境会给出奖励反馈。

3. **状态（State, \(s\)）**：
   - 描述环境在某个时刻的特征。
   - 智能体根据状态感知环境并选择动作。

4. **动作（Action, \(a\)）**：
   - 智能体可以选择的行为。
   - 不同的动作会导致环境状态的改变以及不同的奖励。

5. **奖励（Reward, \(r\)）**：
   - 环境对智能体某次动作的即时反馈。
   - 奖励正值表示好的行为（鼓励），负值表示不好的行为（惩罚）。

6. **策略（Policy, \(\pi\)）**：
   - 决定智能体如何选择动作的规则或函数。
   - 可以是概率分布（策略网络），也可以是直接映射关系（Q表）。

7. **价值函数（Value Function）**：
   - 用来评估某个状态或状态-动作对的长期价值。
   - 通常通过估计未来的累积奖励来衡量。

---

## 核心目标
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/e827082dbe8b458b8a565e3fc243ed7b.png)


## 特点

1. **试错学习**：
   - 强化学习通过反复尝试不同动作并从环境中获取反馈来调整行为。

2. **延迟奖励**：
   - 行为的效果可能需要经过一段时间才能体现，智能体需要学会权衡短期和长期奖励。

3. **目标导向**：
   - 强化学习的目标是最大化长期累积奖励，而不仅仅关注即时奖励。

4. **交互式学习**：
   - 强化学习的学习过程是智能体与环境的不断交互。

---

## 基本算法

强化学习的算法可以分为以下几类：

#### 1. **值函数方法**
- 通过学习 **状态值函数（Value Function）** 或 **状态-动作值函数（Q值）** 来指导决策。
- **代表算法**：
  - Q-Learning：基于离散状态和动作的更新公式，逐步学出最优策略。
  - SARSA：基于智能体当前动作和环境反馈更新 Q 值。

#### 2. **策略方法**
- 直接优化策略（Policy），使得策略可以直接输出动作的概率分布。
- **代表算法**：
  - 策略梯度（Policy Gradient）：直接对策略函数的参数进行优化。
  - REINFORCE：一种基于蒙特卡洛的策略梯度方法。

#### 3. **演员-评论家方法（Actor-Critic）**
- 同时学习值函数和策略，结合了值函数方法和策略方法的优点。
- **代表算法**：
  - A3C（Asynchronous Advantage Actor-Critic）
  - PPO（Proximal Policy Optimization）


# Q学习
Q学习是属于强化学习的一种算法，Q学习要去学习的其实就是强化学习要去做的事情

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/25d1167d0f814ff797bb4ce0fab2ac57.png)
实例：走迷宫

要在一个神秘的迷宫中寻找宝藏。迷宫中有多个房间，每个房间都有一个宝箱，里面可能有宝藏，也可能是陷阱。你的目标是找到宝藏并躲避陷阱，以获取尽可能多的奖励。

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/b8580f51a5074f45941ab14b48180a51.png)
在Q学习中，你会记录每个房间的状态、每个行动的奖励以及你对每个行动的期望价值，这个期望价值被称为Q值。你会根据当前的状态和Q值选择一个行动，并与环境互动，得到下一个状态和对应的奖励。

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/109df79b15d0459881a786dc3f8a93e7.png)

在每次互动后，你会更新之前状态下所选择了的行动的Q值，使用一个称为贝尔曼方程的公式。这个公式会考虑之前选择了的行动后的奖励和当前状态的Q值和之前状态的Q值，以更新当前状态下的Q值。通过不断地在迷宫中试错，并不断地更新Q值，你的决策策略会逐渐变得越来越优化。

随着你在迷宫中不断探索，你会逐渐学会哪些行动是最优的，如何避免陷阱，并朝着找到宝藏的最佳路径前进。最终，你可能会成为一个真正的迷宫大师，能够在最短的时间内找到宝藏，避免陷阱。

Q学习是一种强大而灵活的算法，可以用于许多复杂的问题，如自动驾驶、机器人控制和游戏智能等。
#  Q表
Q表是Q学习中的一种数据结构，用于存储不同状态和行动之间的Q值

实例：走迷宫

假设你是一名冒险家，正在探险一个迷宫。迷宫中有多个房间，每个房间都有不同的状态，比如说你所处的位置、周围的环境等。而 你可以采取不同的行动，比如说向左走、向右走、停留等。
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/e327a84cc2604cd59ed026f5213556a6.png)
Q表就像是一个二维表格，其中的每一行代表一个状态，每一列代表一个行动。表格的每个单元格里存储着该状态下执行该行动的Q值，表示这个行动在当前状态下的预期价值。Q值是一个数字，表示执行某个行动后可能获得的累积奖励
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/eae44ad049df4d60ac1dcf5fb9d66dbc.png)
在开始探险之前，Q表中的Q值通常会被初始化为一些初始值，比如0。随着探险的进行，你会根据实际的互动经验来不断更新Q表中的Q值。

 ![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/324563b665ce49058fc3f87c493b7175.png)
当你在迷宫中要选择一个行动时，你可以查找当前状态对应的行，然后选择具有最高Q值的列对应的行动作为你的决策。
        
#  Bellman方程式
先理解整体更新公式，Q-Learning的本质是要更准确的预测出来Q值，而预测Q值这件事情我们是不是可以把它看作是一个回归任务，回归任务最常用的loss损失函数就是MSE

 
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/0992622f4d0c421192b90fa02c23f1bf.png)
看出调整参数Q值，根据MSE梯度下降法更新参数可以得到 ：新参数值=原始参数值+超参数*（y实际值-y预期值）

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/08544d532a724463aa134116d8df9ead.png)
这个公式可以简单解释为：当前状态下选择某个行动的Q值等于当前即时奖励加上折扣后的下一状态的最优行动的Q值。即当前状态下选择行动时，同时考虑即时奖励和未来奖励，并选择能够使得总体预期价值最大化的行动