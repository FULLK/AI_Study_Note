@[toc]
# 全连接层
## 卷积神经网络的作用
卷积神经网络（Convolutional Neural Network），多用于图像识别，但不仅仅用于图像识别。不过我们在学习卷积神经网络的过程中，可以把图像识别当成假想任务，理解起来会更直观一些。

## 全连接层的问题

 **为什么图像数据从多维（图像的原始格式）转化为一维向量（供全连接神经网络处理）时，为何会丢失空间和通道的相关信息，以及可能隐藏的模式信息。**

### 场景
想象我们正在处理一张包含蓝天和绿色草地的照片，天空和草地之间有一个清晰的分界线。

### 图像处理和数据转换
1. **多维数据（图像格式）**：
   - 在图像中，蓝天部分的像素可能具有高蓝色值，例如RGB(135, 206, 235)。
   - 草地的像素可能是绿色的，例如RGB(34, 139, 34)。
   - 在图像格式中，这些像素彼此相邻，形成了可视的界限和模式，如天空和草地的分界。

2. **转换为一维向量**：
   - 当这张图像被转换为一维向量以输入到全连接神经网络时，所有像素都会被展开成一个长长的数字序列。这个序列简单地将每个像素的RGB值依次排列。
   - 例如，如果图像是100x100像素，那么向量将是一个长度为30000的数组（每个像素3个颜色通道）。

### 信息丢失的实例
1. **丢失的空间相关性**：
   - 在原始图像中，相邻的像素（如天空中的相邻蓝色像素或草地中的绿色像素）展示了高度的空间相关性。这种相关性有助于我们理解和解释图像的结构（如天空通常在上方，草地在下方）。
   - 转换为一维向量后，这些空间上的关联性丢失了。向量中的值只是色彩信息的简单连续，而不保留任何像素之间原有的物理邻近关系。

2. **忽略的形状信息和模式**：
   - 原图中的形状和模式，如天空与草地的交界线，是视觉上的关键信息，可能揭示了图像的某些重要特征或物体的边界。
   - 当这些数据被转换为一维向量时，所有这些形状信息和模式都被降维处理，使得全连接网络难以从这种一维表示中重新识别或学习到这些重要的空间特征。


![](https://i-blog.csdnimg.cn/direct/43697c2fecd043749d2b13452e283ba2.png)
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/41ffdb026adf45b98ebce5fc189c7e51.png)
为了解决上述问题，我们引入卷积神经网络进行特征提取，既能提取到相邻像素点之间的特征模式，又能保证参数的个数不随图片尺寸变化。一个典型的卷积神经网络结构，多层卷积和池化层组合作用在输入图片上，在网络的最后通常会加入一系列全连接层，ReLU激活函数一般加在卷积或者全连接层的输出上，网络中通常还会加入Dropout来防止过拟合。

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/79f6a1cf211f481c974e225a7b03a2c6.png)
这个图像描述了一个典型的卷积神经网络（CNN）的结构，用于图像识别任务，这里以手写数字识别为例。整个网络分为两大部分：特征提取和分类。

### 特征提取阶段
这一阶段主要由多个卷积层和池化层（下采样层）组成，目的是从输入图像中自动学习到有用的特征。

1. **输入**: 网络接收一个32x32像素的图像（例如，这里的手写数字“3”）。
2. **第一层卷积（C1）**: 使用5x5的卷积核对输入图像进行卷积操作，生成多个特征图（feature maps），这里得到28x28的特征图。卷积层通过滑动卷积核提取图像的局部特征，每个特征图代表图像在不同卷积核作用下的响应。
3. **第一层池化（S1）**: 对卷积后的特征图使用2x2的池化窗口进行下采样，通常用最大池化或平均池化来降低特征图的维度（这里降至14x14），减少计算量并保持特征的主要信息。
4. **第二层卷积（C2）**: 再次使用5x5的卷积核对池化后的特征图进行卷积，进一步提取特征，输出更小尺寸（10x10）的特征图。
5. **第二层池化（S2）**: 对第二层卷积的输出进行池化，进一步降低特征图的尺寸（这里到5x5）。

### 分类阶段
提取到的特征被平展（flatten）成一维向量，并通过一个或多个全连接层（fully connected layers）进行处理，以进行最终的分类。

1. **全连接层**: 特征向量被送入全连接层，全连接层的神经元将学习特征之间的复杂关系，输出每个类别（这里是数字0-9）的得分。
2. **输出层**: 最后的输出层通常使用softmax激活函数，将得分转换为概率，每个数字对应一个概率。

整个网络通过反向传播算法训练，优化卷积核和全连接层中的权重，以提高分类的准确性。CNN特别适合图像处理，因为它们可以自动地从图像中学习有用的局部特征，而不需要手动特征工程。

# 卷积层
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/c3ea0479803449759bfd610967929dd7.png)
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/73ec41a3ecee4ad99f0dcaa749cd5b7f.png)

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/9cbc7f68d9174408b2ccedcd7086f776.png)
# 卷积层的计算
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/1c668af4b73247e2a0ba7ae1d0e01cff.png)
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/ebeb16ffb5ef4654a1f6062ef03f1a25.png)
# 多输入通道场景
 ![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/fa98ae1007bb4a2bac2d2963eae22067.png)
 ![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/c460f5f3fb22454fbab7452dd78b2f53.png)
# 多输出通道场景
 ![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/f589ea8f602a4f0d87d6979452ac5c6d.png)
 ![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/a99a3e0b801c4782832869be29e0163b.png)
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/7c080d267b8347e9878f2e79de4f0652.png)
## 批量操作
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/533bdad62c5d4098aae56c1cd17f6f24.png)
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/5d49a3b05e22420ebd468ca3f57ec146.png)
# 感受野
 ![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/3aaeab560a32486797d0d54cc108ce4a.png)
 ![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/866945ff1dbd42d88a0876fef48f4b64.png)

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/968d3301f8ac4279b826551f7492f071.png)
#  填充（padding）
 ![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/ef85266e1d90413d816ee9bbc83cc400.png)
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/4574a5ab46834cb48c830f61f09de3de.png)
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/1a3378d291a54d34b6a1751200dc9a5d.png)
卷积核为奇数方便计算使得卷积之后图像尺寸不变的填充大小
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/6a4d94fa1aba4ad1a13f7b60b63ec428.png)
# VALID 与 SAME
 
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/9088d06727e84ec5a5007b4c0b3cc099.png)
- 在“有效”填充模式中，不添加任何额外的padding。这意味着卷积核仅在输入数据的有效边界内进行操作，不越过边缘。因此，卷积后的输出尺寸通常会比输入尺寸小。
- 如图所示，使用步长为3和卷积核大小为5的设置，最后一个元素不能进行卷积处理，因为它没有足够的空间完成整个卷积操作，这导致一部分输入数据被丢弃。

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/aa516e14f01a4486a4e09d6f4958e3fa.png)

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/3e5c320ed81742ff9ad3deb31f8dcc56.png)


“相同”填充模式下，通过适当地添加padding来保持输出特征图的尺寸与输入特征图除步长相同（定义）这通常通过在输入的两边均匀添加足够的零来完成。
在图中，卷积核每次移动时都有适当的空间进行操作，包括在输入的开始和结束添加了零填充。这确保了每个输入元素都可以被卷积核覆盖，但当步长为1时same填充模式才会保证从而保持输入和输出尺寸相同。

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/8857b253feff418c9968543b488c17d7.png)
# stride
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/62595f2494264828b46c19a26dcfeb2e.png)
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/bad0c6e946e243f68f1d16f6d1a47c8b.png)

# 池化
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/951bee5ea1c74bdf8bbea52981b73852.png)
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/2d0b478efc9a4ff887f431133723e81e.png)
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/b749488b5ce24a1da4535fb836a9838b.png)
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/136413250d0545c68e70fef97a940680.png)

## 池化的作用
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/b8cd028aeb6e4d3c96f7f506df6032e9.png)
池化操作提高模型对图像小的平移变化的鲁棒性，主要原因在于它减少了每个被池化区域内的细节水平，只保留了最显著的特征（如最大值或平均值）。这种方法降低了模型对输入数据中细微位置改变的敏感性。具体来说，这种鲁棒性是通过以下方式实现的：

1. **局部不变性**：
   - 当使用最大池化时，只有区域内的最大值会被保留，其他的背景信息和不那么显著的特征被忽略。例如，如果一个区域内有一个眼睛的部分和其他面部特征，无论眼睛具体在这个小区域的哪个位置，只要它是最明显的特征，它就会被选取出来。
   - 这意味着，如果眼睛稍微向左或向右移动，只要它仍然是那个小区域内最显著的特征，最大池化仍然会选择它。

2. **降维效应**：
   - 通过将大量像素的输出缩减到较小的输出代表，池化操作有效地减少了数据的维度。这种降维意味着相邻的像素间的轻微变动（如轻微平移）对整体输出的影响被缓和，因为整个区域被总结为一个单一的输出值。

3. **统计特征摘要**：
   - 在平均池化中，由于它计算的是区域内所有值的平均，因此对小的局部变化（如图像的轻微平移）不太敏感。平均值反映了区域内的一般特征而不是具体的位置变化，使得输出对于输入的小幅度变动更为稳定。

# CNN架构
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/7f78de6afea444438239e0573f70c196.png)
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/7e34d76133034188994fa0e4a63dbeae.png)
